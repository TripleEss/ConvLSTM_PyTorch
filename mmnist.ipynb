{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MovingMnistDataset(Dataset):\n",
    "    def __init__(self, path=\"./mnist_test_seq.npy\"):\n",
    "        self.data = np.load(path)\n",
    "        # (t, N, H, W) -> (N, t, C, H, W)\n",
    "        self.data = self.data.transpose(1, 0, 2, 3)[:, :, None, ...]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.data[i, :10, ...]/255).astype(np.float32), (self.data[i, 10:, ...]/255).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = MovingMnistDataset()\n",
    "\n",
    "train_index, valid_index = train_test_split(range(len(dataset)), test_size=0.3)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = Subset(dataset, train_index)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "valid_dataset   = Subset(dataset, valid_index)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) 2020 Masafumi Abeta. All Rights Reserved.\n",
    "Released under the MIT license\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels,\n",
    "                 kernel_size, stride=1, image_size=None):\n",
    "        \"\"\"ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_channels: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: int or (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        stride: int or (int, int)\n",
    "            Stride of the convolution.\n",
    "        image_size: (int, int)\n",
    "            Shape of image.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "\n",
    "        # No bias for hidden, since bias is included in observation convolution\n",
    "        # Pad the hidden layer so that the input and output sizes are equal\n",
    "        self.Wxi = Conv2dStaticSamePadding(\n",
    "            self.in_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size)\n",
    "        self.Whi = Conv2dStaticSamePadding(\n",
    "            self.hidden_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size, bias=False)\n",
    "        self.Wxf = Conv2dStaticSamePadding(\n",
    "            self.in_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size)\n",
    "        self.Whf = Conv2dStaticSamePadding(\n",
    "            self.hidden_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size, bias=False)\n",
    "        self.Wxg = Conv2dStaticSamePadding(\n",
    "            self.in_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size)\n",
    "        self.Whg = Conv2dStaticSamePadding(\n",
    "            self.hidden_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size, bias=False)\n",
    "        self.Wxo = Conv2dStaticSamePadding(\n",
    "            self.in_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size)\n",
    "        self.Who = Conv2dStaticSamePadding(\n",
    "            self.hidden_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=image_size, bias=False)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            4-D Tensor of shape (b, c, h, w).\n",
    "        hs: tuple\n",
    "            Previous hidden state of shape (h_0, c_0).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            h_next, c_next\n",
    "        \"\"\"\n",
    "\n",
    "        h_prev, c_prev = hidden_state\n",
    "        i = torch.sigmoid(self.Wxi(x) + self.Whi(h_prev))\n",
    "        f = torch.sigmoid(self.Wxf(x) + self.Whf(h_prev))\n",
    "        o = torch.sigmoid(self.Wxo(x) + self.Who(h_prev))\n",
    "        g = torch.tanh(self.Wxg(x) + self.Whg(h_prev))\n",
    "\n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels,\n",
    "                 kernel_size, stride=1, image_size=None):\n",
    "        \"\"\"ConvLSTM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_channels: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: int or (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        stride: int or (int, int)\n",
    "            Stride of the convolution.\n",
    "        image_size: (int, int)\n",
    "            Shape of image.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.lstm_cell = ConvLSTMCell(\n",
    "            self.in_channels, self.hidden_channels, self.kernel_size, self.stride, image_size=self.image_size)\n",
    "\n",
    "    def forward(self, xs, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        xs: torch.Tensor\n",
    "            5-D Tensor of shape (b, t, c, h, w).\n",
    "        hs: list\n",
    "            Previous hidden state of shape (h_0, c_0).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            last_state_list, layer_output\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, sequence_length, _, height, width = xs.size()\n",
    "\n",
    "        if hidden_state is None:\n",
    "            hidden_state = (torch.zeros(batch_size, self.hidden_channels, height, width, device=xs.device),\n",
    "                            torch.zeros(batch_size, self.hidden_channels, height, width, device=xs.device))\n",
    "\n",
    "        output_list = []\n",
    "        for t in range(sequence_length):\n",
    "            hidden_state = self.lstm_cell(xs[:, t, ...], hidden_state)\n",
    "            h, _ = hidden_state\n",
    "            output_list.append(h)\n",
    "\n",
    "        output = torch.stack(output_list, dim=1)\n",
    "\n",
    "        return output, hidden_state\n",
    "\n",
    "\n",
    "class Conv2dStaticSamePadding(nn.Conv2d):\n",
    "    \"\"\"2D Convolutions like TensorFlow's 'SAME' mode, with the given input image size.\n",
    "       The padding mudule is calculated in construction function, then used in forward.\n",
    "\n",
    "        # Copyright: lukemelas (github username)\n",
    "        # Released under the MIT License <https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/LICENSE>\n",
    "        # <https://github.com/lukemelas/EfficientNet-PyTorch/blob/4d63a1f77eb51a58d6807a384dda076808ec02c0/efficientnet_pytorch/utils.py>\n",
    "    \"\"\"\n",
    "\n",
    "    # With the same calculation as Conv2dDynamicSamePadding\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, image_size=None, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, **kwargs)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [\n",
    "            self.stride[0]] * 2\n",
    "\n",
    "        # Calculate padding based on image size and save it\n",
    "        assert image_size is not None\n",
    "        ih, iw = (image_size, image_size) if isinstance(\n",
    "            image_size, int) else image_size\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] +\n",
    "                    (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] +\n",
    "                    (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            self.static_padding = nn.ZeroPad2d(\n",
    "                (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "        else:\n",
    "            self.static_padding = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.static_padding(x)\n",
    "        x = F.conv2d(x, self.weight, self.bias, self.stride,\n",
    "                     self.padding, self.dilation, self.groups)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvLSTMEncoderPredictor(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        \"\"\"ConvLSTM Encoder Predictor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_size: (int, int)\n",
    "            Shape of image.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_1 = ConvLSTM(\n",
    "            in_channels=1, hidden_channels=32, kernel_size=3, stride=1, image_size=image_size)\n",
    "        self.encoder_2 = ConvLSTM(\n",
    "            in_channels=32, hidden_channels=32, kernel_size=3, stride=1, image_size=image_size)\n",
    "        self.encoder_3 = ConvLSTM(\n",
    "            in_channels=32, hidden_channels=32, kernel_size=3, stride=1, image_size=image_size)\n",
    "\n",
    "        self.predictor_1 = ConvLSTM(\n",
    "            in_channels=32, hidden_channels=32, kernel_size=3, stride=1, image_size=image_size)\n",
    "        self.predictor_2 = ConvLSTM(\n",
    "            in_channels=32, hidden_channels=32, kernel_size=3, stride=1, image_size=image_size)\n",
    "        self.predictor_3 = ConvLSTM(\n",
    "            in_channels=32, hidden_channels=32, kernel_size=3, stride=1, image_size=image_size)\n",
    "\n",
    "        self.conv2d = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, hidden_state_1 = self.encoder_1(x)\n",
    "        x, hidden_state_2 = self.encoder_2(x)\n",
    "        x, hidden_state_3 = self.encoder_3(x)\n",
    "\n",
    "        x, _ = self.predictor_1(torch.zeros_like(x), hidden_state_1)\n",
    "        x, _ = self.predictor_2(x, hidden_state_2)\n",
    "        x, _ = self.predictor_3(x, hidden_state_3)\n",
    "\n",
    "        seq_output = []\n",
    "        for t in range(x.shape[1]):\n",
    "            tmp = self.conv2d(x[:, t, :, :, :])\n",
    "            seq_output.append(tmp)\n",
    "        output = torch.stack(seq_output, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer, loss_fn, device):\n",
    "    net.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    for i, (xx, yy) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device).view(-1)\n",
    "\n",
    "        y_pred = net(xx).view(-1)\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def eval_net(net, valid_loader, loss_fn, device):\n",
    "    net.eval()\n",
    "    score = 0\n",
    "\n",
    "    for i, (x, y) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device).view(-1)\n",
    "        with torch.no_grad():\n",
    "            y_pred = net(x).view(-1)\n",
    "        score += loss_fn(y_pred, y).item()\n",
    "\n",
    "    valid_score = score / (i + 1)\n",
    "    return valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(epoch, net, optimizer, scheduler, train_losses, valid_losses, elapsed_time, save_model_path, save_log_path):\n",
    "\n",
    "    now = datetime.datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if save_model_path != None:\n",
    "        os.makedirs(save_model_path, exist_ok=True)\n",
    "        torch.save(net.state_dict(), os.path.join(save_model_path, f\"weight_{now}.pth\"))\n",
    "        print( \"Save model : \" + os.path.join(save_model_path, f\"weight_{now}.pth\") )\n",
    "        torch.save(optimizer.state_dict(), os.path.join(save_model_path, f\"optimizer_{now}.pth\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(save_model_path, f\"scheduler_{now}.pth\"))\n",
    "        print( \"Save optimizer : \" + os.path.join(save_model_path, f\"optimizer_{now}.pth\") )\n",
    "\n",
    "    if save_log_path != None:\n",
    "        if os.path.exists(os.path.join(save_log_path, \"log.csv\")):\n",
    "            # 過去のログ読み込み\n",
    "            log_df = pd.read_csv(os.path.join(save_log_path, \"log.csv\"))\n",
    "        else:\n",
    "            log_df = pd.DataFrame([], columns=[\"datetime\", \"epoch\", \"train_loss\", \"valid_loss\", \"elapsed_time\"])\n",
    "\n",
    "        tmp_log = pd.DataFrame([[now, epoch, train_losses[-1], valid_losses[-1], elapsed_time]], columns=[\"datetime\", \"epoch\", \"train_loss\", \"valid_loss\", \"elapsed_time\"])\n",
    "        log_df = pd.concat([log_df, tmp_log])\n",
    "        log_df.to_csv(os.path.join(save_log_path, \"log.csv\"), index=False)\n",
    "        print( \"Save log : \" + os.path.join(save_log_path, \"log.csv\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "n_iter = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = ConvLSTMEncoderPredictor(image_size=(64, 64)).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.005, betas=(0.9, 0.999))\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    start = time.time()\n",
    "\n",
    "    train_score = train(net, train_dataloader, optimizer, loss_fn, device)\n",
    "    train_losses.append(train_score)\n",
    "\n",
    "    valid_score = eval_net(net, valid_dataloader, loss_fn, device)\n",
    "    valid_losses.append(valid_score)\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print(f\"epoch:{epoch}\", \n",
    "            \"--\", \"train loss:{:.5f}\".format(train_losses[-1]),\n",
    "            \"--\",\"valid loss:{:.5f}\".format(valid_losses[-1]),\n",
    "            \"--\", \"elapsed_time:{:.2f}\".format(elapsed_time) + \"[sec]\", flush=True)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        save(epoch, net, optimizer, None, train_losses, valid_losses, elapsed_time, save_model_path=None, save_log_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
